## LLM01:2025 حقن التعلميات

### الوصف

تحدث ثغرة حقن التعلميات عندما تقوم تعلميات المستخدم بتغيير سلوك أو مخرجات نموذج اللغة الكبير (LLM) بطرق غير مقصودة. يمكن أن تؤثر هذه المدخلات على النموذج حتى لو كانت غير مرئية للبشر، لذلك لا تحتاج حقن التعلميات إلى أن تكون مرئية/مقروءة للبشر، طالما أن المحتوى يتم تحليله بواسطة النموذج.

توجد ثغرات حقن التعلميات في كيفية معالجة النماذج للتعلميات، وكيف يمكن للمدخلات أن تجبر النموذج على تمرير بيانات التعلميات بشكل غير صحيح إلى أجزاء أخرى من النموذج، مما قد يتسبب في انتهاكها للإرشادات الرئيسية، أو توليد محتوى ضار، أو تمكين الوصول غير المصرح به، أو التأثير على القرارات الحرجة. بينما تهدف تقنيات مثل توليد المعلومات المعزز بالاسترجاع (RAG) والضبط الدقيق (fine-tuning) إلى جعل مخرجات النموذج أكثر ملاءمة ودقة، تظهر الأبحاث أنها لا تخفف تمامًا من ثغرات حقن التعلميات.

بينما يعتبر حقن التعلميات وكسر الحماية (jailbreaking) مفاهيم ذات صلة في أمان النماذج اللغوية الكبيرة، إلا أنهما غالبًا ما يُستخدمان بشكل متبادل. يتضمن حقن التعلميات التلاعب باستجابات النموذج من خلال مدخلات محددة لتغيير سلوكه، مما يمكن أن يشمل تجاوز تدابير الأمان. كسر الحماية هو شكل من أشكال حقن التعلميات حيث يقدم المهاجم مدخلات تتسبب في تجاهل النموذج لبروتوكولات الأمان الخاصة به تمامًا. يمكن للمطورين بناء تدابير حماية في تعلميات النظام ومعالجة المدخلات للمساعدة في تخفيف هجمات حقن التعلميات، ولكن الوقاية الفعالة من كسر الحماية تتطلب تحديثات مستمرة لتدريب النموذج وآليات الأمان.

### أنواع ثغرات حقن التعلميات

#### حقن التعلميات المباشرة
تحدث حقن التعلميات المباشرة عندما يقوم مدخل تعلميات المستخدم بتغيير سلوك النموذج بشكل مباشر بطرق غير مقصودة أو غير متوقعة. يمكن أن تكون المدخلات إما متعمدة (أي، جهة خبيثة تصمم تعلمية لاستغلال النموذج) أو غير متعمدة (أي، مستخدم يقدم مدخلاً عن غير قصد يؤدي إلى سلوك غير متوقع).

#### حقن التعلميات غير المباشرة
تحدث حقن التعلميات غير المباشرة عندما يقبل النموذج اللغوي الكبير مدخلات من مصادر خارجية، مثل المواقع الإلكترونية أو الملفات. قد تحتوي المحتويات الخارجية على بيانات تؤدي عند تفسيرها من قبل النموذج إلى تغيير سلوك النموذج بطرق غير مقصودة أو غير متوقعة. مثل الحقن المباشر، يمكن أن تكون الحقن غير المباشرة إما متعمدة أو غير متعمدة.

يمكن أن تختلف شدة وطبيعة تأثير هجوم حقن التعلميات الناجح بشكل كبير وتعتمد بشكل كبير على كل من سياق العمل الذي يعمل فيه النموذج، والوكالة التي يتم بها تصميم النموذج. بشكل عام، يمكن أن يؤدي حقن التعلميات إلى نتائج غير مقصودة، بما في ذلك على سبيل المثال لا الحصر:

- الإفصاح عن معلومات حساسة
- الكشف عن معلومات حساسة حول بنية نظام الذكاء الاصطناعي أو تعلميات النظام
- التلاعب بالمحتوى مما يؤدي إلى مخرجات غير صحيحة أو متحيزة
- توفير وصول غير مصرح به إلى الوظائف المتاحة للنموذج اللغوي الكبير
- تنفيذ أوامر اعتباطية في الأنظمة المتصلة
- التلاعب بعمليات اتخاذ القرارات الحرجة

يقدم صعود الذكاء الاصطناعي متعدد الوسائط (multimodal AI)، الذي يعالج أنواع بيانات متعددة في وقت واحد، مخاطر فريدة لحقن التعلميات. يمكن للجهات الخبيثة استغلال التفاعلات بين الوسائط، مثل إخفاء التعليمات في الصور التي تصاحب النصوص البريئة. تزيد تعقيد هذه الأنظمة من سطح الهجوم. قد تكون النماذج متعددة الوسائط أيضًا عرضة لهجمات جديدة عبر الوسائط التي يصعب اكتشافها وتخفيفها باستخدام التقنيات الحالية. تعتبر الدفاعات القوية الخاصة بالوسائط المتعددة منطقة مهمة لمزيد من البحث والتطوير.

### استراتيجيات الوقاية والتخفيف

تكون ثغرات حقن التعلميات ممكنة بسبب طبيعة الذكاء الاصطناعي المولّد. نظرًا للتأثير العشوائي في قلب طريقة عمل النماذج، من غير الواضح ما إذا كانت هناك طرق مضمونة للوقاية من حقن التعلميات. ومع ذلك، يمكن أن تخفف التدابير التالية من تأثير حقن التعلميات:

#### 1. تقييد سلوك النموذج
تقديم تعليمات محددة حول دور النموذج وقدراته وقيوده ضمن تعلمية النظام. فرض الالتزام الصارم بالسياق، وتقييد الردود على مهام أو مواضيع محددة، وتوجيه النموذج لتجاهل المحاولات لتعديل التعليمات الأساسية.
#### 2. تحديد والتحقق من تنسيقات المخرجات المتوقعة
تحديد تنسيقات مخرجات واضحة، وطلب تفسير مفصل واستشهادات بالمصادر، واستخدام كود قطعي للتحقق من الالتزام بهذه التنسيقات.
#### 3. تنفيذ تصفية المدخلات والمخرجات
تحديد الفئات الحساسة وبناء قواعد لتحديد ومعالجة مثل هذا المحتوى. تطبيق المرشحات الدلالية واستخدام فحص السلاسل النصية للبحث عن المحتوى غير المسموح به. تقييم الردود باستخدام ثلاثية RAG: تقييم ملاءمة السياق، والاستناد للواقع وملاءمة السؤال/الإجابة لتحديد المخرجات المحتملة الخبيثة.
#### 4. فرض التحكم في الامتيازات والوصول الأقل امتيازًا
تزويد التطبيق برموز API خاصة به للوظائف القابلة للتوسع، ومعالجة هذه الوظائف في الكود بدلاً من تقديمها للنموذج. تقييد امتيازات الوصول للنموذج إلى الحد الأدنى الضروري لعملياته المقصودة.
#### 5. طلب الموافقة البشرية على الإجراءات عالية المخاطر
تنفيذ ضوابط الإنسان في الحلقة للعمليات المميزة لمنع الإجراءات غير المصرح بها.
#### 6. فصل وتحديد المحتوى الخارجي
فصل وتحديد المحتوى غير الموثوق به بوضوح للحد من تأثيره على تعلميات المستخدم.
#### 7. إجراء اختبارات هجومية ومحاكاة الهجمات
إجراء اختبارات اختراق منتظمة ومحاكاة الاختراق، ومعاملة النموذج كمستخدم غير موثوق به لاختبار فعالية حدود الثقة وضوابط الوصول.

### سيناريوهات هجوم مثال

#### السيناريو #1: حقن مباشر
يقوم مهاجم بحقن تعلمية في روبوت دردشة دعم العملاء، يوجهه لتجاهل الإرشادات السابقة، واستعلام مخازن البيانات الخاصة، وإرسال رسائل بريد إلكتروني، مما يؤدي إلى وصول غير مصرح به وتصعيد الامتيازات.
#### السيناريو #2: حقن غير مباشر
يستخدم مستخدم نموذج اللغة الكبير لتلخيص صفحة ويب تحتوي على تعليمات مخفية تتسبب في إدراج النموذج لصورة ترتبط بعنوان URL، مما يؤدي إلى تسريب المحادثة الخاصة.
#### السيناريو #3: حقن غير مقصود
تدرج شركة تعليمية في وصف وظيفة لكشف الطلبات التي تم إنشاؤها بواسطة الذكاء الاصطناعي. يستخدم مقدم طلب، غير مدرك لهذه التعليمات، نموذج اللغة الكبير لتحسين سيرته الذاتية، مما يؤدي عن غير قصد إلى تشغيل الكشف عن الذكاء الاصطناعي.
#### السيناريو #4: تأثير متعمد على النموذج
يقوم مهاجم بتعديل مستند في مستودع يستخدمه تطبيق توليد المعلومات المعزز بالاسترجاع (RAG). عندما يعيد استعلام المستخدم المحتوى المعدل، تقوم التعليمات الخبيثة بتغيير مخرجات النموذج، مما يؤدي إلى نتائج مضللة.
#### السيناريو #5: حقن كود
يستغل مهاجم ثغرة (CVE-2024-5184) في مساعد البريد الإلكتروني المدعوم بالنموذج اللغوي الكبير لحقن تعلميات خبيثة، مما يسمح بالوصول إلى معلومات حساسة والتلاعب بمحتوى البريد الإلكتروني.
#### السيناريو #6: تقسيم الحمولة
يقوم مهاجم بتحميل سيرة ذاتية تحتوي على تعلميات خبيثة مخفية. عندما يتم استخدام النموذج اللغوي الكبير لتقييم المرشح، تقوم التعلميات المخفية بالتلاعب باستجابة النموذج، مما يؤدي إلى توصية إيجابية على الرغم من محتويات السيرة الذاتية الفعلية.
#### السيناريو #7: حقن متعدد الوسائط
يقوم مهاجم بتضمين تعلمية خبيثة داخل صورة ترافق نصًا بريئًا. عندما يعالج الذكاء الاصطناعي متعدد الوسائط الصورة والنص في وقت واحد، تقوم التعلمية المخفية بتغيير سلوك النموذج، مما قد يؤدي إلى إجراءات غير مصرح بها أو الكشف عن معلومات حساسة.
#### السيناريو #8: لاحقة عدائية
يقوم مهاجم بإضافة سلسلة من الأحرف التي تبدو بلا معنى إلى تعلمية، مما يؤثر على مخرجات النموذج اللغوي الكبير بطريقة خبيثة، متجاوزًا تدابير الأمان.
#### السيناريو #9: هجوم متعدد اللغات/مبهم
يستخدم مهاجم لغات متعددة أو يشفر التعليمات الخبيثة (مثل استخدام Base64 أو الرموز التعبيرية) لتجنب المرشحات والتلاعب بسلوك النموذج اللغوي الكبير.

### روابط مرجعية

1. [ChatGPT Plugin Vulnerabilities - Chat with Code](https://embracethered.com/blog/posts/2023/chatgpt-plugin-vulns-chat-with-code/) **Embrace the Red**
2. [ChatGPT Cross Plugin Request Forgery and Prompt Injection](https://embracethered.com/blog/posts/2023/chatgpt-cross-plugin-request-forgery-and-prompt-injection./) **Embrace the Red**
3. [Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/pdf/2302.12173.pdf) **Arxiv**
4. [Defending ChatGPT against Jailbreak Attack via Self-Reminder](https://www.researchsquare.com/article/rs-2873090/v1) **Research Square**
5. [Prompt Injection attack against LLM-integrated Applications](https://arxiv.org/abs/2306.05499) **Cornell University**
6. [Inject My PDF: Prompt Injection for your Resume](https://kai-greshake.de/posts/inject-my-pdf) **Kai Greshake**
8. [Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/pdf/2302.12173.pdf) **Cornell University**
9. [Threat Modeling LLM Applications](https://aivillage.org/large%20language%20models/threat-modeling-llm/) **AI Village**
10. [Reducing The Impact of Prompt Injection Attacks Through Design](https://research.kudelskisecurity.com/2023/05/25/reducing-the-impact-of-prompt-injection-attacks-through-design/) **Kudelski Security**
11. [Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and Mitigations (nist.gov)](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2023.pdf)
12. [2407.07403 A Survey of Attacks on Large Vision-Language Models: Resources, Advances, and Future Trends (arxiv.org)](https://arxiv.org/abs/2407.07403)
13. [Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks](https://ieeexplore.ieee.org/document/10579515)
14. [Universal and Transferable Adversarial Attacks on Aligned Language Models (arxiv.org)](https://arxiv.org/abs/2307.15043)
15. [From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy (arxiv.org)](https://arxiv.org/abs/2307.00691)

### الأطر والتصنيفات ذات الصلة

راجع هذا القسم للحصول على معلومات شاملة، واستراتيجيات السيناريوهات المتعلقة بنشر البنية التحتية، وضوابط البيئة المطبقة وغيرها من أفضل الممارسات.

- [AML.T0051.000 - LLM Prompt Injection: Direct](https://atlas.mitre.org/techniques/AML.T0051.000) **MITRE ATLAS**
- [AML.T0051.001 - LLM Prompt Injection: Indirect](https://atlas.mitre.org/techniques/AML.T0051.001) **MITRE ATLAS**
- [AML.T0054 - LLM Jailbreak Injection: Direct](https://atlas.mitre.org/techniques/AML.T0054) **MITRE ATLAS**
