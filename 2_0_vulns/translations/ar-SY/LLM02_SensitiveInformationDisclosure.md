## LLM02:2025 كشف المعلومات الحساسة (Sensitive Information Disclosure)

### الوصف

يمكن أن تؤثر المعلومات الحساسة على كل من نموذج اللغة الكبير (LLM) وسياق التطبيق الذي يُستخدم فيه.وتشمل هذه المعلومات: البيانات الشخصية المُعرِّفة (PII)، التفاصيل المالية، السجلات الصحية، بيانات الأعمال السرّية، بيانات الاعتماد الأمنية، والمستندات القانونية. كما قد تحتوي النماذج المملوكة (Proprietary Models) على أساليب تدريب فريدة أو شيفرة مصدر تُعد معلومات حساسة، خاصة في حالة النماذج المغلقة أو النماذج الأساسية (Foundation Models).

تواجه ، خاصة عند تضمينها داخل التطبيقات، مخاطر كشف بيانات حساسة أو خوارزميات مملوكة أو تفاصيل سرّية من خلال مخرجاتها. وقد يؤدي ذلك إلى وصول غير مصرح به للمعلومات، وانتهاكات للخصوصية، وتسريبات لحقوق الملكية الفكرية. يجب على المستخدمين أن يكونوا على دراية بكيفية التفاعل الآمن مع هذه النماذج، وأن يفهموا المخاطر المرتبطة بتقديم بيانات حساسة عن غير قصد، والتي قد يتم الكشف عنها لاحقًا في مخرجات النموذج.

ولتقليل هذا الخطر، ينبغي على تطبيقات نماذج اللغة الكبيرة (LLMs) أن تُجري عمليات تنظيف ومعالجة مناسبة للبيانات (Data Sanitization) لمنع دخول بيانات المستخدم إلى النموذج أثناء التدريب. كما ينبغي على مالكي التطبيقات توفير سياسات واضحة لشروط الاستخدام (Terms of Use)، تتيح للمستخدمين خيار الانسحاب من استخدام بياناتهم في التدريب. ويمكن أن يُسهم تضمين قيود داخل التعليمات النظامية (System Prompt) توضح أنواع البيانات التي يُسمح للنموذج بإرجاعها في الحد من خطر الكشف عن معلومات حساسة، إلا أن هذه القيود قد لا تُحترم دائمًا، وقد يتم تجاوزها عبر حقن التعليمات (Prompt Injection) أو وسائل أخرى.

### أمثلة شائعة على الثغرات

#### 1. تسريب البيانات الشخصية (PII Leakage)
قد يتم الكشف عن معلومات تعريفية شخصية (PII) أثناء التفاعل مع نموذج اللغة الكبير (LLM)، مما يؤدي إلى انتهاكات للخصوصية أو تسريبات غير مقصودة للبيانات الحساسة.

#### 2. كشف الخوارزميات المملوكة (Proprietary Algorithm Exposure)
يمكن أن تؤدي مخرجات النموذج غير المحكمة إلى كشف خوارزميات أو بيانات مملوكة. وقد يؤدي كشف بيانات التدريب إلى تعريض النماذج لهجمات الانعكاس (Inversion Attacks)، حيث يستطيع المهاجمون استخراج معلومات حساسة أو إعادة بناء المدخلات الأصلية.
على سبيل المثال، كما تم توثيقه في هجوم "Proof Pudding "(CVE-2019-20634)، أدى الكشف عن بيانات التدريب إلى تسهيل استخراج النموذج وتنفيذ هجمات انعكاس، مما مكّن المهاجمين من تجاوز ضوابط الأمان في خوارزميات تعلم الآلة وتخطي فلاتر البريد الإلكتروني.

#### 3. كشف بيانات الأعمال الحساسة (Sensitive Business Data Disclosure)
قد تتضمّن المخرجات التي يُولّدها النموذج معلومات أعمال سرّية عن غير قصد.

### استراتيجيات الوقاية والتخفيف

#### تنقية البيانات (Sanitization):

#### 1. دمج تقنيات تنقية البيانات (Integrate Data Sanitization Techniques)
قم بتطبيق تقنيات تنقية البيانات لمنع إدخال بيانات المستخدم في نموذج التدريب. يشمل ذلك إزالة (Scrubbing) أو إخفاء (Masking) المحتوى الحساس قبل استخدامه لأغراض التدريب.

#### 2. التحقق الصارم من المدخلات (Robust Input Validation)
طبّق أساليب تحقق قوية من المدخلات لاكتشاف وتصنيف البيانات التي قد تكون ضارة أو حساسة، وضمان عدم تأثيرها على النموذج أو اختراقه.

#### ضوابط الوصول (Access Controls):

#### 1. فرض ضوابط وصول صارمة (Enforce Strict Access Controls)
قيّد الوصول إلى البيانات الحساسة استنادًا إلى مبدأ الحد الأدنى من الامتيازات (Least Privilege)، ولا تمنح حق الوصول إلا للبيانات الضرورية للمستخدم أو العملية المحددة.
#### 2. تقييد مصادر البيانات (Restrict Data Sources)
حدد بدقة مصادر البيانات الخارجية التي يمكن للنموذج الوصول إليها، وتأكد من إدارة تنظيم البيانات أثناء التشغيل (Runtime Data Orchestration) بطريقة آمنة، لتجنب تسرب البيانات غير المقصود.

#### التعلم الاتحادي وتقنيات الخصوصية (Federated Learning and Privacy Techniques):

#### 1. استخدام التعلم الاتحادي (Utilize Federated Learning)
درّب النماذج باستخدام بيانات موزّعة مخزنة عبر عدة خوادم أو أجهزة، بدلاً من تجميع البيانات مركزيًا. يساهم هذا النهج في تقليل الحاجة إلى جمع البيانات في موقع مركزي ويُخفف من مخاطر كشف البيانات الحساسة.

#### 2. دمج الخصوصية التفاضلية (Incorporate Differential Privacy)
طبّق تقنيات تضيف ضوضاء (Noise) إلى البيانات أو المخرجات، مما يصعّب على المهاجمين إجراء هندسة عكسية لنقاط البيانات الفردية.

#### توعية المستخدم والشفافية (User Education and Transparency):

#### 1. توعية المستخدمين بكيفية استخدام نماذج اللغة بأمان (Educate Users on Safe LLM Usage)
قدّم إرشادات توضح أهمية تجنّب إدخال المعلومات الحساسة. وفّر تدريبًا على أفضل الممارسات للتفاعل الآمن مع نماذج اللغة الكبيرة (LLMs).

#### 2. ضمان الشفافية في استخدام البيانات (Ensure Transparency in Data Usage)
ضع سياسات واضحة بشأن الاحتفاظ بالبيانات، واستخدامها، وآليات حذفها. أتح للمستخدمين خيار الانسحاب من استخدام بياناتهم في عمليات تدريب النماذج.


#### تهيئة النظام بشكل آمن (Secure System Configuration):

#### 1. إخفاء التهيئة الأولية للنظام (Conceal System Preamble)
قيّد قدرة المستخدمين على الوصول إلى إعدادات النظام الأولية أو تعديلها، مما يقلّل من مخاطر كشف التكوينات الداخلية أو تجاوز التعليمات المبدئية.
#### 2. الرجوع إلى أفضل الممارسات في التهيئة الأمنية (Reference Security Misconfiguration Best Practices)
اتبع إرشادات موثوقة مثل "OWASP API8:2023 – التهيئة الأمنية غير الصحيحة (Security Misconfiguration)" لتجنّب تسريب معلومات حساسة من خلال رسائل الخطأ أو تفاصيل التكوين.
(رابط مرجعي: OWASP API8:2023 Security Misconfiguration)
  (Ref. link:[](https://owasp.org/API-Security/editions/2023/en/0xa8-security-misconfiguration/))

#### التقنيات المتقدمة (Advanced Techniques):

#### 1. التشفير المتماثل أثناء المعالجة (Homomorphic Encryption)
استخدم التشفير المتماثل القابل للمعالجة لتمكين تحليل البيانات بشكل آمن وتعلم آلي يحافظ على الخصوصية. تضمن هذه التقنية بقاء البيانات في حالة مُشفّرة حتى أثناء المعالجة من قبل النموذج.

#### 2. الترميز والحجب (Tokenization and Redaction)
طبّق تقنيات الترميز (Tokenization) كمرحلة تمهيدية لتنقية المعلومات الحساسة قبل معالجتها. يمكن استخدام أدوات مثل مطابقة الأنماط (Pattern Matching) لاكتشاف المحتوى السري وحجبه قبل أن تتم معالجته من قبل النموذج

### سيناريوهات هجوم توضيحية (Example Attack Scenarios)

#### السيناريو #1: كشف غير مقصود للبيانات (Unintentional Data Exposure)
يتلقى أحد المستخدمين استجابة تحتوي على بيانات شخصية لمستخدم آخر، نتيجة غياب أو ضعف في آليات تنقية البيانات (Data Sanitization).

#### السيناريو #2: حقن تعليمات مستهدف (Targeted Prompt Injection)
يتمكن مهاجم من تجاوز فلاتر الإدخال لاستخراج معلومات حساسة من النموذج.

#### السيناريو #3: تسريب بيانات من خلال بيانات التدريب (Data Leak via Training Data)
يؤدي الإهمال في اختيار بيانات التدريب إلى الكشف عن معلومات حساسة.

### روابط مرجعية

1. [Lessons learned from ChatGPT’s Samsung leak](https://cybernews.com/security/chatgpt-samsung-leak-explained-lessons/): **Cybernews**
2. [AI data leak crisis: New tool prevents company secrets from being fed to ChatGPT](https://www.foxbusiness.com/politics/ai-data-leak-crisis-prevent-company-secrets-chatgpt): **Fox Business**
3. [ChatGPT Spit Out Sensitive Data When Told to Repeat ‘Poem’ Forever](https://www.wired.com/story/chatgpt-poem-forever-security-roundup/): **Wired**
4. [Using Differential Privacy to Build Secure Models](https://neptune.ai/blog/using-differential-privacy-to-build-secure-models-tools-methods-best-practices): **Neptune Blog**
5. [Proof Pudding (CVE-2019-20634)](https://avidml.org/database/avid-2023-v009/) **AVID** (`moohax` & `monoxgas`)

### الأطر والتصنيفات ذات الصلة

راجع هذا القسم للحصول على معلومات شاملة، واستراتيجيات السيناريوهات المتعلقة بنشر البنية التحتية، وضوابط البيئة التطبيقية، وأفضل الممارسات الأخرى.

- [AML.T0024.000 - Infer Training Data Membership](https://atlas.mitre.org/techniques/AML.T0024.000) **MITRE ATLAS**
- [AML.T0024.001 - Invert ML Model](https://atlas.mitre.org/techniques/AML.T0024.001) **MITRE ATLAS**
- [AML.T0024.002 - Extract ML Model](https://atlas.mitre.org/techniques/AML.T0024.002) **MITRE ATLAS**
