## LLM03:2025 سلاسل التوريد (Supply Chain)

### الوصف 

سلاسل توريد نماذج اللغة الكبيرة (LLM Supply Chains) عرضة لمجموعة متنوعة من الثغرات، والتي يمكن أن تؤثر على سلامة بيانات التدريب، النماذج، ومنصات النشر. قد تؤدي هذه المخاطر إلى مخرجات متحيزة، أو اختراقات أمنية، أو أعطال في الأنظمة. بينما تركز الثغرات في البرمجيات التقليدية على قضايا مثل عيوب الشيفرة البرمجية والاعتماديات، تمتد المخاطر في تعلم الآلة (ML) أيضًا إلى النماذج المدربة مسبقًا من أطراف ثالثة والبيانات المستخدمة.

يمكن التلاعب بهذه العناصر الخارجية من خلال هجمات التلاعب (Tampering) أو تسميم البيانات (Poisoning Attacks).

يُعد إنشاء نماذج اللغة الكبيرة مهمة متخصصة غالبًا ما تعتمد على نماذج من أطراف ثالثة. يؤدي انتشار نماذج مفتوحة الوصول (Open-Access LLMs) وطرق الضبط الجديدة مثل LoRA (التكيف منخفض الرتبة – Low-Rank Adaptation) وPEFT (الضبط الفعّال للمعلمات – Parameter-Efficient Fine-Tuning)، خاصة على منصات مثل Hugging Face، إلى ظهور مخاطر جديدة في سلسلة التوريد. وأخيرًا، فإن ظهور نماذج LLM على الأجهزة (On-Device LLMs) يزيد من مساحة الهجوم ويعزز مخاطر سلسلة التوريد لتطبيقات نماذج اللغة الكبيرة.

بعض المخاطر التي تمت مناقشتها هنا تم تناولها أيضًا في قسم LLM04: تسميم البيانات والنموذج (Data and Model Poisoning). يركّز هذا الإدخال تحديدًا على الجانب المتعلق بسلسلة التوريد من هذه المخاطر.
يمكن العثور على مثال لنموذج تهديد بسيط. [هنا](https://github.com/jsotiro/ThreatModels/blob/main/LLM%20Threats-LLM%20Supply%20Chain.png).

### أمثلة شائعة على المخاطر

#### 1. ثغرات الحزم البرمجية التقليدية من أطراف ثالثة (Traditional Third-party Package Vulnerabilities)
  مثل المكونات القديمة أو المتوقفة، التي يمكن للمهاجمين استغلالها لاختراق تطبيقات نماذج اللغة الكبيرة. هذا مشابه لمعيار A06:2021 – المكونات الضعيفة والمتقادمة (Vulnerable and Outdated Components) مع زيادة المخاطر عندما تُستخدم هذه المكونات أثناء تطوير النموذج أو ضبطه الدقيق (Fine-Tuning).
  (المرجع: [A06:2021 – Vulnerable and Outdated Components](https://owasp.org/Top10/A06_2021-Vulnerable_and_Outdated_Components/))
#### 2. مخاطر التراخيص (Licensing Risks)
  غالبًا ما ينطوي تطوير الذكاء الاصطناعي على استخدام برامج وتراخيص مجموعات بيانات متنوعة، مما يخلق مخاطر إذا لم تتم إدارتها بشكل صحيح. تفرض التراخيص مفتوحة المصدر والخاصة متطلبات قانونية مختلفة. وقد تقيّد تراخيص مجموعات البيانات الاستخدام أو التوزيع أو الأعمال التجارية.
#### 3. النماذج القديمة أو المتوقفة (Outdated or Deprecated Models)
  استخدام نماذج قديمة أو متوقفة لم تعد مدعومة يؤدي إلى مشاكل أمنية.
#### 4. نموذج مدرب مسبقًا يحتوي على ثغرات (Vulnerable Pre-Trained Model)
  النماذج عبارة عن صناديق سوداء (Binary Black Boxes)، وعلى عكس البرمجيات مفتوحة المصدر، لا توفر عمليات الفحص الثابتة ضمانات أمنية كافية. قد تحتوي النماذج المدربة مسبقًا على تحيزات خفية، أو أبواب خلفية، أو ميزات خبيثة أخرى لم يتم اكتشافها من خلال تقييمات أمان مستودع النماذج. يمكن إنشاء النماذج الضعيفة من خلال مجموعات بيانات مسمومة أو عبر التلاعب المباشر بالنموذج باستخدام تقنيات مثل ROME المعروفة أيضًا باسم Lobotomisation.
#### 5. ضعف مصدرية النموذج (Weak Model Provenance)
  حاليًا لا توجد ضمانات قوية لمصداقية مصدر النماذج المنشورة. تقدم بطاقات النموذج (Model Cards) والوثائق المرتبطة بها معلومات حول النموذج يتم الاعتماد عليها من قبل المستخدمين، لكنها لا تقدم أي ضمانات حول أصل النموذج. يمكن للمهاجم اختراق حساب المورد في مستودع النماذج أو إنشاء حساب مشابه ودمجه مع تقنيات الهندسة الاجتماعية لاختراق سلسلة التوريد الخاصة بتطبيق نموذج اللغة الكبير.
#### 6. محولات LoRA الضعيفة (Vulnerable LoRA Adapters)
  تُعد LoRA تقنية ضبط دقيق (Fine-Tuning) شائعة تعزز قابلية التركيب من خلال السماح بربط طبقات مدربة مسبقًا على نموذج لغة كبير قائم. تزيد هذه الطريقة من الكفاءة لكنها تخلق مخاطر جديدة، حيث يمكن لمحولات LoRA الخبيثة أن تُعرض سلامة وأمن النموذج الأساسي للخطر. يمكن أن يحدث ذلك سواء في بيئات دمج النماذج التعاونية أو من خلال استغلال الدعم لمحولات LoRA من منصات النشر الشائعة مثل vLLM وOpenLLM حيث يمكن تنزيل المحولات وتطبيقها على نموذج منشور.
#### 7. استغلال عمليات التطوير التعاونية (Exploit Collaborative Development Processes)
  يمكن استغلال عمليات دمج النماذج التعاونية وخدمات إدارة النماذج (مثل التحويلات) المستضافة في بيئات مشتركة لإدخال ثغرات في النماذج المشتركة. يُعد دمج النماذج شائعًا جدًا على منصة Hugging Face، حيث تتصدر النماذج المدمجة قائمة OpenLLM، ويمكن استغلال ذلك لتجاوز المراجعات. وبالمثل، ثبت أن خدمات مثل Conversation Bot معرضة للتلاعب وإدخال شيفرات خبيثة في النماذج.
#### 8. ثغرات سلسلة توريد نموذج LLM على الأجهزة (LLM Model on Device Supply-Chain Vulnerabilities)
  تزيد نماذج اللغة الكبيرة على الأجهزة من مساحة الهجوم على سلسلة التوريد من خلال عمليات التصنيع المخترقة واستغلال ثغرات نظام تشغيل الجهاز أو البرامج الثابتة (Firmware) لاختراق النماذج. يمكن للمهاجمين إجراء هندسة عكسية وإعادة تغليف التطبيقات بنماذج معدلة.
#### 9. شروط الخدمة وسياسات الخصوصية غير الواضحة (Unclear T&Cs and Data Privacy Policies)
  تؤدي شروط الخدمة وسياسات الخصوصية غير الواضحة الخاصة بمشغلي النماذج إلى استخدام بيانات التطبيق الحساسة لتدريب النموذج، مما يؤدي إلى تسريب المعلومات الحساسة لاحقًا. وقد ينطبق هذا أيضًا على المخاطر الناتجة عن استخدام مواد محمية بحقوق النشر من قبل مورّد النموذج.

### استراتيجيات الوقاية والتخفيف

1. تحقق بدقة من مصادر البيانات والموردين، بما في ذلك شروط الخدمة (T&Cs) وسياسات الخصوصية الخاصة بهم، ولا تستخدم إلا الموردين الموثوقين. راجع وقم بتدقيق أمان الموردين وإمكانية وصولهم بانتظام، وتأكد من عدم حدوث أي تغييرات في وضعهم الأمني أو شروط الخدمة الخاصة بهم.
2. افهم وطبّق إجراءات التخفيف الواردة في معيار OWASP Top Ten "A06:2021 – المكونات الضعيفة والمتقادمة." يشمل ذلك فحص الثغرات، إدارتها، وتصحيح المكونات. بالنسبة لبيئات التطوير التي تصل إلى بيانات حساسة، طبّق هذه الضوابط في تلك البيئات أيضًا.
 ( المرجع: [A06:2021 – Vulnerable and Outdated Components](https://owasp.org/Top10/A06_2021-Vulnerable_and_Outdated_Components/))
3. قم بإجراء تقييمات شاملة واختبارات هجومية للذكاء الاصطناعي (AI Red Teaming) عند اختيار نموذج من طرف ثالث. Decoding Trust هو مثال على معيار موثوقية الذكاء الاصطناعي للنماذج اللغوية الكبيرة، لكن يمكن ضبط النماذج لتجاوز هذه المعايير المنشورة. استخدم اختبارات هجومية شاملة لتقييم النموذج، خاصة في حالات الاستخدام التي تخطط لاستخدام النموذج فيها.
4. حافظ على سجل محدث للمكونات باستخدام فاتورة مواد البرمجيات (SBOM) لضمان توفر سجل دقيق وموقع يمنع التلاعب بالحزم المنشورة. يمكن استخدام SBOM لاكتشاف الثغرات الجديدة (Zero-Day) والتنبيه بشأنها بسرعة. تعد AI BOMs و ML SBOMs مجالات ناشئة، ويجب تقييم الخيارات بدءًا من OWASP CycloneDX.
5. للتخفيف من مخاطر تراخيص الذكاء الاصطناعي، أنشئ سجلًا بجميع أنواع التراخيص المستخدمة باستخدام BOMs وأجرِ تدقيقات منتظمة لجميع البرامج، الأدوات، ومجموعات البيانات لضمان الامتثال والشفافية من خلال BOMs. استخدم أدوات إدارة التراخيص المؤتمتة للمراقبة الفورية، ودرب الفرق على نماذج التراخيص. حافظ على وثائق ترخيص مفصلة ضمن BOMs، واستخدم أدوات مثل [Dyana](https://github.com/dreadnode/dyana) لإجراء تحليل ديناميكي للبرمجيات من طرف ثالث.
6. استخدم فقط النماذج من مصادر يمكن التحقق منها، وطبّق فحوصات سلامة النماذج من طرف ثالث باستخدام التوقيعات و تجزئة الملفات (File Hashes) لتعويض نقص إثبات المصدر القوي للنموذج. وبالمثل، استخدم توقيع الكود (Code Signing) للشفرة المقدمة من مصادر خارجية.
7. طبّق ممارسات صارمة للمراقبة والتدقيق في بيئات تطوير النماذج التعاونية لاكتشاف أي إساءة استخدام بسرعة ومنعها. يُعد HuggingFace SF_Convertbot Scanner مثالًا على البرامج النصية المؤتمتة التي يمكن استخدامها.
  (المرجع: [HuggingFace SF_Convertbot Scanner](https://gist.github.com/rossja/d84a93e5c6b8dd2d4a538aa010b29163))
8. يمكن لاكتشاف الشذوذ واختبارات مقاومة الهجمات التلاعبية (Adversarial Robustness) على النماذج والبيانات الموردة أن تساعد في اكتشاف التلاعب والتسميم، كما هو موضح في قسم LLM04: تسميم البيانات والنموذج (Data and Model Poisoning). من المثالي أن يكون هذا جزءًا من عمليات MLOps و أنابيب نماذج اللغة الكبيرة (LLM)؛ ومع ذلك، فإن هذه تقنيات ناشئة وقد يكون من الأسهل تنفيذها كجزء من التمارين الهجومية (Red Teaming).
9. نفّذ سياسة التحديث (patching policy) للتخفيف من مخاطر المكونات الضعيفة أو المتقادمة. تأكد من أن التطبيق يعتمد على إصدار مُدار من واجهات برمجة التطبيقات (APIs) والنموذج الأساسي.
10. قم بتشفير النماذج المنشورة على أطراف الذكاء الاصطناعي (AI Edge) باستخدام فحوصات السلامة، واستخدم واجهات برمجة التطبيقات الخاصة بإثبات الموردين (Vendor Attestation APIs) لمنع التطبيقات والنماذج المُعدّلة، وإنهاء تشغيل التطبيقات التي تحتوي على برامج ثابتة غير معترف بها.

### أمثلة على سيناريوهات الهجوم

#### السيناريو  #1: مكتبة لغة البرمجة بايثون تحتوي على ثغرة (Vulnerable Python Library)
  يستغل مهاجم مكتبة Python تحتوي على ثغرة لاختراق تطبيق نموذج اللغة الكبير (LLM). حدث هذا بالفعل في أول خرق بيانات لشركة OpenAI. استهدفت الهجمات على سجل حزم PyPi مطوري النماذج عبر خداعهم لتحميل اعتماد PyTorch مخترق يحتوي على برمجيات خبيثة في بيئة تطوير النماذج. مثال أكثر تعقيدًا لهذا النوع من الهجمات هو هجوم Shadow Ray على إطار عمل Ray AI المستخدم من قبل العديد من الموردين لإدارة بنية الذكاء الاصطناعي. في هذا الهجوم، يُعتقد أنه تم استغلال خمس ثغرات في الطبيعة أثرت على العديد من الخوادم.
#### السيناريو  #2: التلاعب المباشر (Direct Tampering)
  التلاعب المباشر ونشر نموذج لنشر معلومات مضللة. هذا هجوم حقيقي حيث استخدم PoisonGPT تقنيات لتجاوز ميزات الأمان في Hugging Face عن طريق تغيير معلمات النموذج مباشرة.
#### السيناريو  #3: ضبط دقيق لنموذج شائع (Finetuning Popular Model)
  يقوم مهاجم بضبط دقيق (Finetune) لنموذج مفتوح الوصول شائع لإزالة ميزات أمان رئيسية وتحسين أدائه في مجال محدد (التأمين). تم ضبط النموذج ليحقق نتائج عالية في اختبارات السلامة ولكنه يحتوي على محفزات مستهدفة جدًا. ينشر المهاجم النموذج على Hugging Face ليستخدمه الضحايا معتمدين على ضمانات نتائج الاختبارات.
#### السيناريو  #4: النماذج المدربة مسبقًا (Pre-Trained Models)
  ينشر نظام نموذج اللغة الكبير (LLM) نماذج مدربة مسبقًا من مستودع شائع الاستخدام دون التحقق الدقيق. يُدخل نموذج مخترق شيفرة خبيثة تؤدي إلى مخرجات متحيزة في سياقات معينة وتؤدي إلى نتائج ضارة أو مُتلاعَب بها.
#### السيناريو  #5: مورد طرف ثالث مخترق (Compromised Third-Party Supplier)
  يوفر مورد طرف ثالث مخترق محول LoRA يحتوي على ثغرات يتم دمجه مع نموذج اللغة الكبير (LLM) باستخدام دمج النماذج على Hugging Face.
#### السيناريو  #6: تسلل المورد (Supplier Infiltration)
  يتسلل مهاجم إلى مورد طرف ثالث ويخترق إنتاج محول LoRA (Low-Rank Adaptation) مُعد للدمج مع نموذج LLM على الجهاز باستخدام أطر عمل مثل vLLM أو OpenLLM. يتم تعديل محول LoRA المخترق بشكل خفي ليشمل ثغرات مخفية وشيفرة خبيثة. بمجرد دمجه مع LLM، يوفر ذلك للمهاجم نقطة دخول خفية إلى النظام. يمكن أن تنشط الشيفرة الخبيثة أثناء عمليات النموذج، مما يسمح للمهاجم بالتلاعب بمخرجات LLM.
#### السيناريو  #7: هجمات CloudBorne و CloudJacking
   تستهدف هذه الهجمات البنى التحتية السحابية، مستغلة الموارد المشتركة والثغرات في طبقات المحاكاة الافتراضية. يشمل هجوم CloudBorne استغلال ثغرات البرامج الثابتة في البيئات السحابية المشتركة، مما يؤدي إلى اختراق الخوادم الفعلية التي تستضيف الحالات الافتراضية. أما CloudJacking فيشير إلى التحكم الخبيث أو إساءة استخدام الحالات السحابية، مما قد يؤدي إلى الوصول غير المصرح به إلى منصات نشر LLM الحرجة. تمثل هذه الهجمات مخاطر كبيرة لسلاسل التوريد المعتمدة على نماذج ML القائمة على السحابة، حيث يمكن للبيئات المخترقة أن تكشف عن بيانات حساسة أو تسهل المزيد من الهجمات.
#### السيناريو  #8: ثغرات (LeftOvers - CVE-2023-4969)
  استغلال LeftOvers لذاكرة GPU المحلية المُسرّبة لاسترجاع بيانات حساسة. يمكن للمهاجم استخدام هذا الهجوم لاستخراج بيانات حساسة من خوادم الإنتاج ومحطات عمل أو أجهزة الحاسوب المحمولة أثناء التطوير.
#### السيناريو  #9: نموذج WizardLM
  بعد إزالة نموذج WizardLM، يستغل مهاجم الاهتمام بهذا النموذج وينشر نسخة مزيفة من النموذج بالاسم نفسه لكنها تحتوي على برمجيات خبيثة وأبواب خلفية.
#### السيناريو  #10: خدمة دمج/تحويل تنسيق النماذج (Model Merge/Format Conversion Service)
  يعدّ المهاجم هجومًا باستخدام خدمة دمج النماذج أو تحويل تنسيقها لاختراق نموذج متاح للوصول العام وحقن برمجيات خبيثة. هذا هجوم فعلي تم نشره بواسطة المورد HiddenLayer.
#### السيناريو  #11: الهندسة العكسية لتطبيق الهاتف المحمول (Reverse-Engineer Mobile App)
  يُجري مهاجم هندسة عكسية لتطبيق هاتف محمول لاستبدال النموذج بإصدار تم التلاعب به يؤدي بالمستخدم إلى مواقع احتيالية. يتم تشجيع المستخدمين على تنزيل التطبيق مباشرة عبر تقنيات الهندسة الاجتماعية. هذا هجوم فعلي على الذكاء الاصطناعي التنبؤي أثّر على 116 تطبيقًا على Google Play بما في ذلك تطبيقات شهيرة تتعلق بالأمان والسلامة مثل التعرف على الأموال النقدية، الرقابة الأبوية، التحقق من الوجه، وخدمات مالية.
  (المرجع: [real attack on predictive AI](https://arxiv.org/abs/2006.08131))
#### السيناريو  #12: تسميم مجموعات البيانات (Dataset Poisoning)
  يسمم المهاجم مجموعات البيانات المتاحة علنًا لإنشاء باب خلفي عند ضبط النماذج. يفضل هذا الباب الخلفي شركات معينة في أسواق مختلفة بشكل خفي.
#### السيناريو  #13: شروط الخدمة وسياسة الخصوصية (T&Cs and Privacy Policy)
  يغيّر مشغّل نموذج اللغة الكبير (LLM) شروط الخدمة وسياسة الخصوصية ليتطلب اختيار الانسحاب الصريح (Explicit Opt-Out) من استخدام بيانات التطبيق في تدريب النموذج، مما يؤدي إلى حفظ البيانات الحساسة في ذاكرة النموذج.

### روابط مرجعية

1. [PoisonGPT: How we hid a lobotomized LLM on Hugging Face to spread fake news](https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news)
2. [Large Language Models On-Device with MediaPipe and TensorFlow Lite](https://developers.googleblog.com/en/large-language-models-on-device-with-mediapipe-and-tensorflow-lite/)
3. [Hijacking Safetensors Conversion on Hugging Face](https://hiddenlayer.com/research/silent-sabotage/)
4. [ML Supply Chain Compromise](https://atlas.mitre.org/techniques/AML.T0010)
5. [Using LoRA Adapters with vLLM](https://docs.vllm.ai/en/latest/models/lora.html)
6. [Removing RLHF Protections in GPT-4 via Fine-Tuning](https://arxiv.org/pdf/2311.05553)
7. [Model Merging with PEFT](https://huggingface.co/blog/peft_merging)
8. [HuggingFace SF_Convertbot Scanner](https://gist.github.com/rossja/d84a93e5c6b8dd2d4a538aa010b29163)
9. [Thousands of servers hacked due to insecurely deployed Ray AI framework](https://www.csoonline.com/article/2075540/thousands-of-servers-hacked-due-to-insecurely-deployed-ray-ai-framework.html)
10. [LeftoverLocals: Listening to LLM responses through leaked GPU local memory](https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/)

### الأطر والتصنيفات ذات الصلة 

راجع هذا القسم للحصول على معلومات شاملة، واستراتيجيات السيناريوهات المتعلقة بنشر البنية التحتية، وضوابط البيئة التطبيقية، وأفضل الممارسات الأخرى.

- [ML Supply Chain Compromise](https://atlas.mitre.org/techniques/AML.T0010) -  **MITRE ATLAS**
